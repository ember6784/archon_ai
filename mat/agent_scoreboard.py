"""
Agent Scoreboard - Performance Metrics for Multi-Agent Debates
=============================================================

Tracks agent performance to prevent "infinite hiring" of ineffective agents.
Implements metrics collection, analysis, and auto-actions for underperformers.

Usage:
    from agent_scoreboard import Scoreboard, AgentMetrics

    scoreboard = Scoreboard()
    scoreboard.record_debate("security_expert", outcome={
        "consensus_score": 0.8,
        "tokens_used": 1500,
        "response_time": 3.2,
        "verdict": "approved"
    })

    metrics = scoreboard.get_metrics("security_expert")
    if metrics.cost_efficiency < 0.5:
        scoreboard.disable_agent("security_expert", reason="Low cost efficiency")
"""

import json
import time
import logging
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Dict, List, Optional, Any
from collections import deque
from enum import Enum

logger = logging.getLogger(__name__)


class VerdictType(Enum):
    """–¢–∏–ø –≤–µ—Ä–¥–∏–∫—Ç–∞ –∏–∑ –¥–µ–±–∞—Ç–æ–≤"""
    APPROVED = "approved"
    APPROVED_WITH_RISKS = "approved_with_risks"
    WARNING = "warning"
    REJECTED = "rejected"
    MANUAL_REVIEW = "manual_review"


@dataclass
class AgentMetrics:
    """
    –ú–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
    –æ–± –æ—Ç–∫–ª—é—á–µ–Ω–∏–∏ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤.
    """
    agent_id: str
    template_origin: Optional[str] = None  # –ò–∑ –∫–∞–∫–æ–≥–æ —à–∞–±–ª–æ–Ω–∞ —Å–æ–∑–¥–∞–Ω

    # –£—á–∞—Å—Ç–∏–µ –≤ –¥–µ–±–∞—Ç–∞—Ö
    debates_participated: int = 0
    debates_approved: int = 0
    debates_rejected: int = 0

    # –ö–æ–Ω—Å–µ–Ω—Å—É—Å (—Å–æ–≥–ª–∞—Å–∏–µ —Å —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º)
    consensus_achieved: float = 0.0  # % —Å–æ–≥–ª–∞—Å–∏–π

    # –†–µ—Å—É—Ä—Å—ã
    avg_tokens_per_debate: int = 0
    total_tokens_used: int = 0
    avg_response_time: float = 0.0  # —Å–µ–∫—É–Ω–¥—ã
    total_response_time: float = 0.0

    # –ö–∞—á–µ—Å—Ç–≤–æ
    value_score: float = 0.5  # –û—Ü–µ–Ω–∫–∞ –æ—Ç Auditor'–∞ (0-1)
    veto_rate: float = 0.0    # –ß–∞—Å—Ç–æ—Ç–∞ –Ω–∞–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –≤–µ—Ç–æ

    # –í—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç—å (–¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤)
    survival_rate: float = 1.0  # –°–∫–æ–ª—å–∫–æ –¥–µ–±–∞—Ç–æ–≤ "–≤—ã–∂–∏–ª"

    # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    cost_efficiency: float = 0.5  # value / cost (—Ç–æ–∫–µ–Ω—ã)

    # –°—Ç–∞—Ç—É—Å
    is_active: bool = True
    disabled_reason: Optional[str] = None
    disabled_at: Optional[str] = None

    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    first_seen: str = field(default_factory=lambda: datetime.now().isoformat())
    last_seen: str = field(default_factory=lambda: datetime.now().isoformat())
    last_updated: str = field(default_factory=lambda: datetime.now().isoformat())

    def calculate_cost_efficiency(self) -> float:
        """–ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å cost efficiency"""
        if self.avg_tokens_per_debate == 0:
            return 0.0

        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å = value_score / (tokens / 1000)
        token_cost = self.avg_tokens_per_debate / 1000.0
        self.cost_efficiency = self.value_score / max(token_cost, 0.1)
        return self.cost_efficiency

    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "AgentMetrics":
        """–°–æ–∑–¥–∞—Ç—å –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
        return cls(**data)

    def update_debate(self, outcome: Dict[str, Any]) -> None:
        """
        –û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –¥–µ–±–∞—Ç–∞

        Args:
            outcome: –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–±–∞—Ç–∞
                - consensus_score: float (0-1)
                - tokens_used: int
                - response_time: float (seconds)
                - verdict: str (VerdictType)
                - value_score: float (0-1) - optional
                - veto_applied: bool
        """
        self.debates_participated += 1
        self.last_seen = datetime.now().isoformat()

        # –ö–æ–Ω—Å–µ–Ω—Å—É—Å
        consensus = outcome.get("consensus_score", 0.5)
        self.consensus_achieved = (
            (self.consensus_achieved * (self.debates_participated - 1) + consensus)
            / self.debates_participated
        )

        # –í–µ—Ädict
        verdict = outcome.get("verdict", "unknown")
        if verdict in [VerdictType.APPROVED.value, VerdictType.APPROVED_WITH_RISKS.value]:
            self.debates_approved += 1
        elif verdict == VerdictType.REJECTED.value:
            self.debates_rejected += 1

        # –¢–æ–∫–µ–Ω—ã
        tokens = outcome.get("tokens_used", 0)
        self.total_tokens_used += tokens
        self.avg_tokens_per_debate = (
            (self.avg_tokens_per_debate * (self.debates_participated - 1) + tokens)
            / self.debates_participated
        )

        # –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
        response_time = outcome.get("response_time", 0)
        self.total_response_time += response_time
        self.avg_response_time = self.total_response_time / self.debates_participated

        # Value score
        value = outcome.get("value_score", 0.5)
        self.value_score = (
            (self.value_score * (self.debates_participated - 1) + value)
            / self.debates_participated
        )

        # Veto rate
        if outcome.get("veto_applied", False):
            self.veto_rate = (
                (self.veto_rate * (self.debates_participated - 1) + 1.0)
                / self.debates_participated
            )
        else:
            self.veto_rate = (
                self.veto_rate * (self.debates_participated - 1)
            ) / self.debates_participated

        # –ü–µ—Ä–µ—Å—á–∏—Ç–∞–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        self.calculate_cost_efficiency()
        self.last_updated = datetime.now().isoformat()


@dataclass
class DebateOutcome:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–±–∞—Ç–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ Scoreboard"""
    agent_id: str
    consensus_score: float  # 0-1
    tokens_used: int
    response_time: float  # —Å–µ–∫—É–Ω–¥—ã
    verdict: str  # VerdictType
    value_score: float = 0.5
    veto_applied: bool = False
    debate_id: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class ScoreboardConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Scoreboard"""
    # –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞ (–¥–ª—è –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –ø—É—Ç–µ–π)
    base_dir: Optional[str] = None  # –ï—Å–ª–∏ None - –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

    # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∞–≤—Ç–æ-–¥–µ–π—Å—Ç–≤–∏–π
    min_value_score: float = 0.3      # –ù–∏–∂–µ - –∞–≥–µ–Ω—Ç –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω
    min_cost_efficiency: float = 0.5   # –ù–∏–∂–µ - –æ—Ç–∫–ª—é—á–∏—Ç—å
    max_veto_rate: float = 0.5         # –í—ã—à–µ - –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å –∏–ª–∏ —É–¥–∞–ª–∏—Ç—å
    min_debates_for_evaluation: int = 5  # –ú–∏–Ω–∏–º—É–º –¥–µ–±–∞—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏

    # –•—Ä–∞–Ω–µ–Ω–∏–µ (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ base_dir –∏–ª–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ)
    metrics_file: str = "memory/agent_scoreboard.json"
    history_file: str = "memory/agent_metrics_history.jsonl"

    # –ê–≤—Ç–æ-–¥–µ–π—Å—Ç–≤–∏—è
    auto_disable_low_performers: bool = True
    auto_flag_for_retraining: bool = True

    def get_absolute_path(self, relative_path: str) -> str:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π

        Args:
            relative_path: –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å

        Returns:
            –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å
        """
        path = Path(relative_path)
        if path.is_absolute():
            return str(path)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º base_dir
        if self.base_dir:
            base = Path(self.base_dir)
        else:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
            base = Path(__file__).parent

        return str(base / path)


class Scoreboard:
    """
    Scoreboard –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤

    –§—É–Ω–∫—Ü–∏–∏:
    - –ó–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –¥–µ–±–∞—Ç–∞
    - –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - –ê–≤—Ç–æ-–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤
    - –ò—Å—Ç–æ—Ä–∏—è –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    """

    def __init__(self, config: Optional[ScoreboardConfig] = None):
        self.config = config or ScoreboardConfig()
        self._metrics: Dict[str, AgentMetrics] = {}
        self._history: deque = deque(maxlen=10000)  # –ò—Å—Ç–æ—Ä–∏—è –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        self._load_metrics()
        self._load_history()

        logger.info(f"Scoreboard initialized with {len(self._metrics)} agents")

    def record_debate(self, agent_id: str, outcome: Dict[str, Any]) -> AgentMetrics:
        """
        –ó–∞–ø–∏—Å–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–±–∞—Ç–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–∞

        Args:
            agent_id: ID –∞–≥–µ–Ω—Ç–∞
            outcome: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–µ–±–∞—Ç–∞

        Returns:
            –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∞–≥–µ–Ω—Ç–∞
        """
        # –°–æ–∑–¥–∞—ë–º –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –Ω–µ—Ç
        if agent_id not in self._metrics:
            self._metrics[agent_id] = AgentMetrics(agent_id=agent_id)

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics = self._metrics[agent_id]
        metrics.update_debate(outcome)

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self._history.append({
            "timestamp": datetime.now().isoformat(),
            "agent_id": agent_id,
            "outcome": outcome,
            "metrics_snapshot": {
                "debates_participated": metrics.debates_participated,
                "value_score": metrics.value_score,
                "cost_efficiency": metrics.cost_efficiency
            }
        })

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥–∏ (–∞–≤—Ç–æ-–¥–µ–π—Å—Ç–≤–∏—è)
        if metrics.debates_participated >= self.config.min_debates_for_evaluation:
            self._check_auto_actions(agent_id, metrics)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        self._save_metrics()
        self._save_history()

        return metrics

    def record_debate_batch(self, outcomes: List[Dict[str, Any]]) -> Dict[str, AgentMetrics]:
        """
        –ó–∞–ø–∏—Å–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –¥–µ–±–∞—Ç–∞

        Args:
            outcomes: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å agent_id -> –º–µ—Ç—Ä–∏–∫–∏
        """
        results = {}
        for outcome in outcomes:
            agent_id = outcome.get("agent_id")
            if agent_id:
                results[agent_id] = self.record_debate(agent_id, outcome)

        return results

    def get_metrics(self, agent_id: str) -> Optional[AgentMetrics]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∞–≥–µ–Ω—Ç–∞"""
        return self._metrics.get(agent_id)

    def get_all_metrics(self) -> Dict[str, AgentMetrics]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤"""
        return self._metrics.copy()

    def get_top_performers(self, limit: int = 5, metric: str = "cost_efficiency") -> List[AgentMetrics]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø performers

        Args:
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            metric: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ (cost_efficiency, value_score, consensus_achieved)

        Returns:
            –°–ø–∏—Å–æ–∫ –ª—É—á—à–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤
        """
        active = [m for m in self._metrics.values() if m.is_active]
        sorted_metrics = sorted(
            active,
            key=lambda m: getattr(m, metric, 0),
            reverse=True
        )
        return sorted_metrics[:limit]

    def get_underperformers(self, threshold: float = 0.3) -> List[AgentMetrics]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤

        Args:
            threshold: –ü–æ—Ä–æ–≥ value_score

        Returns:
            –°–ø–∏—Å–æ–∫ –∞–≥–µ–Ω—Ç–æ–≤ —Å value_score –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞
        """
        return [
            m for m in self._metrics.values()
            if m.is_active and m.value_score < threshold
            and m.debates_participated >= self.config.min_debates_for_evaluation
        ]

    def flag_underperformers(self, threshold: float = 0.3) -> List[str]:
        """
        –ü–æ–º–µ—Ç–∏—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è

        Args:
            threshold: –ü–æ—Ä–æ–≥ value_score

        Returns:
            –°–ø–∏—Å–æ–∫ ID –ø–æ–º–µ—á–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤
        """
        flagged = []

        for metrics in self.get_underperformers(threshold):
            self.disable_agent(
                metrics.agent_id,
                reason=f"Low value score: {metrics.value_score:.2f} < {threshold}"
            )
            flagged.append(metrics.agent_id)

        if flagged:
            logger.warning(f"Flagged {len(flagged)} underperforming agents: {flagged}")

        return flagged

    def disable_agent(self, agent_id: str, reason: str) -> None:
        """
        –û—Ç–∫–ª—é—á–∏—Ç—å –∞–≥–µ–Ω—Ç–∞

        Args:
            agent_id: ID –∞–≥–µ–Ω—Ç–∞
            reason: –ü—Ä–∏—á–∏–Ω–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∏—è
        """
        if agent_id in self._metrics:
            metrics = self._metrics[agent_id]
            metrics.is_active = False
            metrics.disabled_reason = reason
            metrics.disabled_at = datetime.now().isoformat()

            logger.info(f"Disabled agent {agent_id}: {reason}")
            self._save_metrics()

    def enable_agent(self, agent_id: str) -> None:
        """–í–∫–ª—é—á–∏—Ç—å –∞–≥–µ–Ω—Ç –æ–±—Ä–∞—Ç–Ω–æ"""
        if agent_id in self._metrics:
            metrics = self._metrics[agent_id]
            metrics.is_active = True
            metrics.disabled_reason = None
            metrics.disabled_at = None

            logger.info(f"Re-enabled agent {agent_id}")
            self._save_metrics()

    def get_history(self, agent_id: str, limit: int = 100) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –º–µ—Ç—Ä–∏–∫ –∞–≥–µ–Ω—Ç–∞

        Args:
            agent_id: ID –∞–≥–µ–Ω—Ç–∞
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π

        Returns:
            –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø–∏—Å–µ–π
        """
        history = [
            h for h in self._history
            if h.get("agent_id") == agent_id
        ]
        return history[-limit:]

    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        all_metrics = list(self._metrics.values())
        active = [m for m in all_metrics if m.is_active]

        if not active:
            return {
                "total_agents": len(all_metrics),
                "active_agents": 0,
                "disabled_agents": len(all_metrics),
            }

        return {
            "total_agents": len(all_metrics),
            "active_agents": len(active),
            "disabled_agents": len(all_metrics) - len(active),
            "avg_value_score": sum(m.value_score for m in active) / len(active),
            "avg_cost_efficiency": sum(m.cost_efficiency for m in active) / len(active),
            "avg_tokens_per_debate": sum(m.avg_tokens_per_debate for m in active) / len(active),
            "total_debates_recorded": sum(m.debates_participated for m in all_metrics),
            "total_tokens_used": sum(m.total_tokens_used for m in all_metrics),
        }

    def _check_auto_actions(self, agent_id: str, metrics: AgentMetrics) -> None:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–≤—Ç–æ-–¥–µ–π—Å—Ç–≤–∏—è"""
        if not self.config.auto_disable_low_performers:
            return

        # –ù–∏–∑–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å - –æ—Ç–∫–ª—é—á–∞–µ–º
        if metrics.cost_efficiency < self.config.min_cost_efficiency:
            self.disable_agent(
                agent_id,
                reason=f"Low cost efficiency: {metrics.cost_efficiency:.2f} < {self.config.min_cost_efficiency}"
            )
            return

        # –ù–∏–∑–∫–∏–π value score - –æ—Ç–∫–ª—é—á–∞–µ–º
        if metrics.value_score < self.config.min_value_score:
            self.disable_agent(
                agent_id,
                reason=f"Low value score: {metrics.value_score:.2f} < {self.config.min_value_score}"
            )
            return

        # –í—ã—Å–æ–∫–∏–π veto rate - –º–µ—Ç–∫–∞ –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        if metrics.veto_rate > self.config.max_veto_rate:
            logger.warning(
                f"Agent {agent_id} has high veto rate: {metrics.veto_rate:.2f}. "
                f"Consider retraining."
            )

    def _save_metrics(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–∞–π–ª"""
        path = Path(self.config.get_absolute_path(self.config.metrics_file))
        path.parent.mkdir(parents=True, exist_ok=True)

        data = {
            agent_id: metrics.to_dict()
            for agent_id, metrics in self._metrics.items()
        }

        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def _load_metrics(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        path = Path(self.config.get_absolute_path(self.config.metrics_file))
        if not path.exists():
            return

        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            for agent_id, metrics_data in data.items():
                self._metrics[agent_id] = AgentMetrics.from_dict(metrics_data)

            logger.info(f"Loaded metrics for {len(self._metrics)} agents")
        except Exception as e:
            logger.error(f"Failed to load metrics: {e}")

    def _save_history(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –≤ —Ñ–∞–π–ª"""
        path = Path(self.config.get_absolute_path(self.config.history_file))
        path.parent.mkdir(parents=True, exist_ok=True)

        with open(path, 'a', encoding='utf-8') as f:
            for entry in list(self._history)[-100:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')

    def _load_history(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∏–∑ —Ñ–∞–π–ª–∞"""
        path = Path(self.config.get_absolute_path(self.config.history_file))
        if not path.exists():
            return

        try:
            with open(path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self._history.append(json.loads(line))

            logger.info(f"Loaded {len(self._history)} history entries")
        except Exception as e:
            logger.error(f"Failed to load history: {e}")


# =============================================================================
# CLI DASHBOARD
# =============================================================================

class ScoreboardDashboard:
    """CLI –¥–∞—à–±–æ—Ä–¥ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –º–µ—Ç—Ä–∏–∫"""

    def __init__(self, scoreboard: Scoreboard):
        self.scoreboard = scoreboard

    def show_overview(self) -> None:
        """–ü–æ–∫–∞–∑–∞—Ç—å –æ–±–∑–æ—Ä–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        stats = self.scoreboard.get_statistics()

        print("\n" + "=" * 70)
        print("AGENT SCOREBOARD - OVERVIEW")
        print("=" * 70)

        print(f"\nüìä General Statistics:")
        print(f"  Total Agents:     {stats['total_agents']}")
        print(f"  Active Agents:    {stats['active_agents']}")
        print(f"  Disabled Agents:  {stats['disabled_agents']}")

        if stats['active_agents'] > 0:
            print(f"\nüìà Performance Metrics:")
            print(f"  Avg Value Score:     {stats['avg_value_score']:.3f}")
            print(f"  Avg Cost Efficiency: {stats['avg_cost_efficiency']:.3f}")
            print(f"  Avg Tokens/Debate:    {stats['avg_tokens_per_debate']:.0f}")

        print(f"\nüìù Total Activity:")
        print(f"  Debates Recorded:  {stats['total_debates_recorded']}")
        print(f"  Tokens Used:       {stats['total_tokens_used']:,}")

        print("\n" + "=" * 70)

    def show_top_performers(self, limit: int = 5) -> None:
        """–ü–æ–∫–∞–∑–∞—Ç—å –ª—É—á—à–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤"""
        top = self.scoreboard.get_top_performers(limit=limit)

        print(f"\nüèÜ TOP {limit} PERFORMERS (by cost efficiency)")
        print("-" * 70)

        for i, metrics in enumerate(top, 1):
            status = "‚úÖ" if metrics.is_active else "‚ùå"
            print(f"{i}. {status} {metrics.agent_id:25s} | "
                  f"value: {metrics.value_score:.2f} | "
                  f"efficiency: {metrics.cost_efficiency:.2f} | "
                  f"debates: {metrics.debates_participated}")

        print()

    def show_underperformers(self, threshold: float = 0.3) -> None:
        """–ü–æ–∫–∞–∑–∞—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤"""
        under = self.scoreboard.get_underperformers(threshold=threshold)

        print(f"\n‚ö†Ô∏è  UNDERPERFORMERS (value_score < {threshold})")
        print("-" * 70)

        if not under:
            print("  No underperformers found!")
        else:
            for metrics in under:
                print(f"  ‚ùå {metrics.agent_id:25s} | "
                      f"value: {metrics.value_score:.2f} | "
                      f"efficiency: {metrics.cost_efficiency:.2f} | "
                      f"debates: {metrics.debates_participated}")

        print()

    def show_agent_details(self, agent_id: str) -> None:
        """–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≥–µ–Ω—Ç–µ"""
        metrics = self.scoreboard.get_metrics(agent_id)

        if not metrics:
            print(f"\n‚ùå Agent '{agent_id}' not found")
            return

        print(f"\nüìã AGENT DETAILS: {agent_id}")
        print("-" * 70)

        print(f"\nüìä Participation:")
        print(f"  Debates Participated:  {metrics.debates_participated}")
        print(f"  Approved:             {metrics.debates_approved}")
        print(f"  Rejected:             {metrics.debates_rejected}")

        print(f"\nüìà Performance:")
        print(f"  Consensus Achieved:   {metrics.consensus_achieved:.2%}")
        print(f"  Value Score:          {metrics.value_score:.2f}")
        print(f"  Cost Efficiency:      {metrics.cost_efficiency:.2f}")
        print(f"  Veto Rate:            {metrics.veto_rate:.2%}")

        print(f"\nüí∞ Resources:")
        print(f"  Avg Tokens/Debate:    {metrics.avg_tokens_per_debate:.0f}")
        print(f"  Total Tokens:         {metrics.total_tokens_used:,}")
        print(f"  Avg Response Time:    {metrics.avg_response_time:.2f}s")

        print(f"\nüìÖ Timeline:")
        print(f"  First Seen:  {metrics.first_seen}")
        print(f"  Last Seen:   {metrics.last_seen}")

        if not metrics.is_active:
            print(f"\n‚ùå STATUS: DISABLED")
            print(f"  Reason:   {metrics.disabled_reason}")
            print(f"  At:       {metrics.disabled_at}")
        else:
            print(f"\n‚úÖ STATUS: ACTIVE")

        print()

    def show_leaderboard(self) -> None:
        """–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—ã–π leaderboard"""
        all_metrics = list(self.scoreboard._metrics.values())
        sorted_metrics = sorted(
            all_metrics,
            key=lambda m: m.cost_efficiency,
            reverse=True
        )

        print(f"\nüìä FULL LEADERBOARD ({len(sorted_metrics)} agents)")
        print("=" * 70)
        print(f"{'Rank':<5} {'Agent':<25} {'Value':<7} {'Eff':<7} {'Debates':<8} {'Status':<7}")
        print("-" * 70)

        for i, metrics in enumerate(sorted_metrics, 1):
            status = "Active" if metrics.is_active else "Disabled"
            print(f"{i:<5} {metrics.agent_id:<25} "
                  f"{metrics.value_score:<7.2f} "
                  f"{metrics.cost_efficiency:<7.2f} "
                  f"{metrics.debates_participated:<8} "
                  f"{status:<7}")

        print()


# =============================================================================
# INTEGRATION WITH DebateStateMachine
# =============================================================================

class ScoreboardIntegration:
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Scoreboard —Å DebateStateMachine

    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –¥–µ–±–∞—Ç–∞.
    """

    def __init__(self, scoreboard: Scoreboard):
        self.scoreboard = scoreboard

    def record_debate_outcome(
        self,
        debate_id: str,
        participants: List[str],
        outcome: Dict[str, Any]
    ) -> None:
        """
        –ó–∞–ø–∏—Å–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–±–∞—Ç–∞ –¥–ª—è –≤—Å–µ—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤

        Args:
            debate_id: ID –¥–µ–±–∞—Ç–∞
            participants: –°–ø–∏—Å–æ–∫ ID —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
            outcome: –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–±–∞—Ç–∞
        """
        consensus = outcome.get("consensus_score", 0.5)
        verdict = outcome.get("verdict", "unknown")

        for agent_id in participants:
            # –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π outcome –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
            agent_outcome = {
                "consensus_score": consensus,
                "tokens_used": outcome.get(f"{agent_id}_tokens", 1000),
                "response_time": outcome.get(f"{agent_id}_time", 5.0),
                "verdict": verdict,
                "value_score": outcome.get("value_score", 0.5),
                "veto_applied": outcome.get(f"{agent_id}_veto", False),
                "debate_id": debate_id
            }

            self.scoreboard.record_debate(agent_id, agent_outcome)


# =============================================================================
# CONVENIENCE FUNCTIONS
# =============================================================================

_global_scoreboard: Optional[Scoreboard] = None


def get_scoreboard() -> Scoreboard:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π Scoreboard"""
    global _global_scoreboard
    if _global_scoreboard is None:
        _global_scoreboard = Scoreboard()
    return _global_scoreboard


def record_agent_performance(agent_id: str, outcome: Dict[str, Any]) -> AgentMetrics:
    """–ó–∞–ø–∏—Å–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞"""
    scoreboard = get_scoreboard()
    return scoreboard.record_debate(agent_id, outcome)


def get_agent_metrics(agent_id: str) -> Optional[AgentMetrics]:
    """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∞–≥–µ–Ω—Ç–∞"""
    scoreboard = get_scoreboard()
    return scoreboard.get_metrics(agent_id)


__all__ = [
    # Core
    "AgentMetrics",
    "DebateOutcome",
    "ScoreboardConfig",
    "Scoreboard",
    "ScoreboardDashboard",
    "ScoreboardIntegration",
    # Convenience
    "get_scoreboard",
    "record_agent_performance",
    "get_agent_metrics"
]


# =============================================================================
# TESTS
# =============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    print("=" * 70)
    print("AGENT SCOREBOARD TESTS")
    print("=" * 70)

    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
    import os
    test_metrics_file = "memory/test_agent_scoreboard.json"
    test_history_file = "memory/test_agent_metrics_history.jsonl"

    if os.path.exists(test_metrics_file):
        os.remove(test_metrics_file)
    if os.path.exists(test_history_file):
        os.remove(test_history_file)

    # –¢–µ—Å—Ç 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    print("\n[Test 1] Initialization...")
    config = ScoreboardConfig(
        metrics_file=test_metrics_file,
        history_file=test_history_file,
        min_debates_for_evaluation=3  # –î–ª—è —Ç–µ—Å—Ç–æ–≤
    )
    scoreboard = Scoreboard(config)
    print(f"  ‚úì Scoreboard created")

    # –¢–µ—Å—Ç 2: –ó–∞–ø–∏—Å—å –¥–µ–±–∞—Ç–æ–≤
    print("\n[Test 2] Recording debates...")
    for i in range(10):
        scoreboard.record_debate("security_expert", {
            "consensus_score": 0.8 + (i % 3) * 0.1,
            "tokens_used": 1000 + i * 100,
            "response_time": 2.0 + i * 0.1,
            "verdict": "approved" if i % 2 == 0 else "approved_with_risks",
            "value_score": 0.7 + (i % 4) * 0.1,
            "veto_applied": i == 5
        })

    metrics = scoreboard.get_metrics("security_expert")
    assert metrics.debates_participated == 10
    assert metrics.value_score > 0.5
    print(f"  ‚úì Recorded 10 debates")
    print(f"    Value score: {metrics.value_score:.2f}")
    print(f"    Cost efficiency: {metrics.cost_efficiency:.2f}")

    # –¢–µ—Å—Ç 3: Dashboard
    print("\n[Test 3] Dashboard...")
    dashboard = ScoreboardDashboard(scoreboard)
    dashboard.show_agent_details("security_expert")

    # –¢–µ—Å—Ç 4: Top performers
    print("\n[Test 4] Top performers...")
    scoreboard.record_debate("performance_guru", {
        "consensus_score": 0.9,
        "tokens_used": 500,
        "response_time": 1.5,
        "verdict": "approved",
        "value_score": 0.95
    })
    dashboard.show_top_performers(limit=3)

    # –¢–µ—Å—Ç 5: Disable underperformer
    print("\n[Test 5] Disable underperformer...")
    scoreboard.record_debate("low_performer", {
        "consensus_score": 0.2,
        "tokens_used": 5000,
        "response_time": 10.0,
        "verdict": "rejected",
        "value_score": 0.1
    })
    # –î–æ–±–∞–≤–ª—è–µ–º –µ—â—ë —á—Ç–æ–±—ã –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –ø–æ—Ä–æ–≥
    for _ in range(config.min_debates_for_evaluation):
        scoreboard.record_debate("low_performer", {
            "consensus_score": 0.2,
            "tokens_used": 5000,
            "response_time": 10.0,
            "verdict": "rejected",
            "value_score": 0.1
        })

    metrics = scoreboard.get_metrics("low_performer")
    print(f"  Low performer is_active: {metrics.is_active}")
    print(f"  Low performer disabled_reason: {metrics.disabled_reason}")

    print("\n" + "=" * 70)
    print("All tests passed!")
    print("=" * 70)

    # Cleanup
    if os.path.exists(test_metrics_file):
        os.remove(test_metrics_file)
    if os.path.exists(test_history_file):
        os.remove(test_history_file)
